{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical work 3\n",
    "Author: Romain Claret\n",
    "## Exercice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_classes = ['x0','x1','x2','x3','x4','x5','x6','x7','x8','x9','y','drop']\n",
    "dataset = pd.read_csv('ex2-system-a.csv', delimiter=';', header=None, names=dataset_classes)\n",
    "dataset.drop('drop', axis=1, inplace=True)\n",
    "#print(dataset.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bayes_classify(data_previous):\n",
    "    return np.argmax(data_previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate:  0.1073\n"
     ]
    }
   ],
   "source": [
    "def get_error(dataset):\n",
    "    wins = 0\n",
    "    for i in range(dataset.shape[0]):\n",
    "        row = dataset.loc[i:i].values[0]\n",
    "        guess = bayes_classify(row[0:-1])\n",
    "        if guess == row[-1]:\n",
    "            wins = wins + 1\n",
    "    return (dataset.shape[0] - wins)/dataset.shape[0]\n",
    "\n",
    "print(\"error rate: \", get_error(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compute confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 944    0   11    0    0    2   10    7    5    1]\n",
      " [   0 1112    2    3    1    4    3    1    9    0]\n",
      " [  10    6  921   12   15    3   19   15   26    5]\n",
      " [   1    1   31  862    2   72    5   14   12   10]\n",
      " [   2    3    6    2  910    1   12    6    4   36]\n",
      " [  12    3    6   29   19  768   19    9   21    6]\n",
      " [  14    3   21    2   22   28  865    0    3    0]\n",
      " [   0   14   30    9    7    2    1  929    3   33]\n",
      " [  12   16   18   26   24   46   22   19  772   19]\n",
      " [  10    4    6   22   53   18    0   48    4  844]]\n"
     ]
    }
   ],
   "source": [
    "def get_confused_matrix(dataset):\n",
    "    features = dataset.shape[1]-1\n",
    "    confused_matrix = np.zeros((features,features),dtype=int)\n",
    "\n",
    "    for i in range(dataset.shape[0]):\n",
    "        row = dataset.loc[i:i].values[0]\n",
    "        guess = bayes_classify(row[0:-1])\n",
    "        truth = np.int(row[-1])\n",
    "        confused_matrix[truth][guess] = confused_matrix[truth][guess]+1\n",
    "        \n",
    "    return confused_matrix\n",
    "    \n",
    "print(get_confused_matrix(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) best classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst precision: 1\n",
      "worst recall: 1\n",
      "best precision: 5\n",
      "best recall: 8\n"
     ]
    }
   ],
   "source": [
    "def get_precision_and_recall(confused_matrix):\n",
    "    TP = [confused_matrix[i,i] for i in range(confused_matrix.shape[0])]\n",
    "    FN = [sum(confused_matrix[i,:]) - TP[i] for i in range(confused_matrix.shape[0])]\n",
    "    FP = [sum(confused_matrix[:,i]) - TP[i] for i in range(confused_matrix.shape[0])]\n",
    "\n",
    "    precision = [TP[i]/(TP[i] + FP[i]) for i in range(confused_matrix.shape[0])]\n",
    "    recall = [TP[i]/(TP[i] + FN[i]) for i in range(confused_matrix.shape[0])]\n",
    "    \n",
    "    return [precision,recall]\n",
    "\n",
    "confused_matrix = get_confused_matrix(dataset)\n",
    "result = get_precision_and_recall(confused_matrix)\n",
    "print(\"worst precision:\", np.argsort(result[0])[-1])\n",
    "print(\"worst recall:\", np.argsort(result[1])[-1])\n",
    "\n",
    "print(\"best precision:\", np.argsort(result[0])[0])\n",
    "print(\"best recall:\", np.argsort(result[1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) do the same with ex1-system-b.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_classes_b = ['x0','x1','x2','x3','x4','x5','x6','x7','x8','x9','y','drop']\n",
    "dataset_b = pd.read_csv('ex2-system-b.csv', delimiter=';', header=None, names=dataset_classes_b)\n",
    "dataset_b.drop('drop', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(precision, recall):\n",
    "    return np.mean([2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) for i in range(len(precision))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst precision: 1\n",
      "worst recall: 1\n",
      "best precision: 3\n",
      "best recall: 5\n"
     ]
    }
   ],
   "source": [
    "confused_matrix_b = get_confused_matrix(dataset_b)\n",
    "result_b = get_precision_and_recall(confused_matrix_b)\n",
    "print(\"worst precision:\", np.argsort(result_b[0])[-1])\n",
    "print(\"worst recall:\", np.argsort(result_b[1])[-1])\n",
    "\n",
    "print(\"best precision:\", np.argsort(result_b[0])[0])\n",
    "print(\"best recall:\", np.argsort(result_b[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_a f1 score: 0.8907308492877297\n",
      "dataset_b f1 score: 0.9608568150389065\n",
      "dataset_a error: 0.1073\n",
      "dataset_b error: 0.0387\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset_a f1 score:\",get_f1(result[0], result[1]))\n",
    "print(\"dataset_b f1 score:\",get_f1(result_b[0], result_b[1]))\n",
    "\n",
    "print(\"dataset_a error:\",get_error(dataset))\n",
    "print(\"dataset_b error:\",get_error(dataset_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on those results, dataset_b looks better than dataset_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
